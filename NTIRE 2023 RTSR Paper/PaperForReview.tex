% CVPR 2023 Paper Template
% based on the CVPR template provided by Ming-Ming Cheng (https://github.com/MCG-NKU/CVPR_Template)
% modified and extended by Stefan Roth (stefan.roth@NOSPAMtu-darmstadt.de)

\documentclass[10pt,twocolumn,letterpaper]{article}

%%%%%%%%% PAPER TYPE  - PLEASE UPDATE FOR FINAL VERSION
\usepackage[review]{cvpr}      % To produce the REVIEW version
%\usepackage{cvpr}              % To produce the CAMERA-READY version
%\usepackage[pagenumbers]{cvpr} % To force page numbers, e.g. for an arXiv version

% Include other packages here, before hyperref.
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{bm}


% It is strongly recommended to use hyperref, especially for the review version.
% hyperref with option pagebackref eases the reviewers' job.
% Please disable hyperref *only* if you encounter grave issues, e.g. with the
% file validation for the camera-ready version.
%
% If you comment hyperref and then uncomment it, you should delete
% ReviewTempalte.aux before re-running LaTeX.
% (Or just hit 'q' on the first LaTeX run, let it finish, and you
%  should be clear).
\usepackage[pagebackref,breaklinks,colorlinks]{hyperref}


% Support for easy cross-referencing
\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}


%%%%%%%%% PAPER ID  - PLEASE UPDATE
\def\cvprPaperID{*****} % *** Enter the CVPR Paper ID here
\def\confName{CVPR}
\def\confYear{2023}


\begin{document}

%%%%%%%%% TITLE - PLEASE UPDATE
\title{Branchy parallelized U-net structure 

Branching, finetune, reparameterized, pruning

paralleled 
branched

Pruning reparameterized super-resolution model}
%多路径

\author{First Author\\
Institution1\\
Institution1 address\\
{\tt\small firstauthor@i1.org}
% For a paper whose authors are all at the same institution,
% omit the following lines up until the closing ``}''.
% Additional authors and addresses can be added with ``\and'',
% just like the second author.
% To save space, use either the email address or home page, not both
\and
Second Author\\
Institution2\\
First line of institution2 address\\
{\tt\small secondauthor@i2.org}
}
\maketitle

%%%%%%%%% ABSTRACT
\begin{abstract}
  Single image super-resolution (SISR) is a computer vision technique that uses deep convolution neural networks to reconstruct high-resolution images from low-resolution images. It has been used in many applications, but due to power constraints on mobile devices, efficient image super-resolution (EISR) is becoming increasingly important and researchers are designing lightweight networks through methods like network pruning, parallelism, and knowledge distillation.
  Efficient network design is key to achieving better performance in EISR, and many designs adopt the U-Net architecture. By deconstructing its branching structure, the main structure of the U-Net can be transformed into mutually independent branches, increasing parallelism and reducing the number of layers executed serially in the network to improve inference speed within the GPU's capacity.
  This paper proposed Parallel RFDN (PRFDN) based on the pre-trained RFDN, which is a typical EISR model with a backbone similar to U-Net. 
  Our method disentangles the sequentially computed trunks in RFDN into branches and performs re-parametrization to make these branches inference in parallel on single devices. 
  After that, we further perform pruning on the model and finetune it to achieve higher performance.
  %TODO: 补充一句实验结果
\end{abstract}

%%%%%%%%% BODY TEXT
\section{Introduction}
\label{sec:intro}

Single image super-resolution (SISR) is a computer vision technique to reconstruct high-resolution images from low-resolution images. Deep convolution neural networks have shown impressive performance in this area and many deep learning-based methods have been proposed to address this ill-posed problem.
This technique has been used in many applications (e.g. video gaming\cite{10.1145/2751496.2751502} and video streaming\cite{kimNeuralEnhancedLiveStreaming2020,yeoNeuralAdaptiveContentaware2018,maoNeuralAdaptiveVideo2017}) to achieve displays with high resolution and high refresh rates and some of the related applications have been deployed on lots of mobile devices\cite{yeoNEMOEnablingNeuralenhanced2020,mehtaEVRNetEfficientVideo2021}.
However, mobile phones have a power-constrained design with a dedicated power supply, which makes efficient image super-resolution (EISR) a growing interest.

Efficient network design is key to achieving better performance in EISR.
To make the network more efficient for resource-limited devices, researchers are designing generic lightweight networks with reduced parameters through methods like network pruning\cite{liuSplitSREndtoEndApproach2021,leeMobiSREfficientOnDevice2019,fang2023depgraph}, parallelism\cite{sandlerMobileNetV2InvertedResiduals2019,howardMobileNetsEfficientConvolutional2017,liuSplitSREndtoEndApproach2021,kongClassSRGeneralFramework2021} and knowledge distillation\cite{liu2020residual,khaniRealTimeVideoInference2021,taoCompressionGenerativePretrained2022,zhangDataFreeKnowledgeDistillation2021}.
However, when the model size becomes sufficiently small, these methods fail to further increase the frame rate of super-resolution. This is because reducing the computational scale of each layer of the model beyond the point where the GPU is fully loaded does not improve the inference speed of the model, if the length of serially executed layers cannot be reduced.
Therefore, lots of EISR designs\cite{liNTIRE2022Challenge2022,duFastMemoryEfficientNetwork2022,mehtaEVRNetEfficientVideo2021} involve careful consideration of network architecture, design, and utilization of limited features to generate more representative features for super-resolution, which can use fewer layers to achieve a good super-resolution quality.

Moreover, recent years have witnessed the wide use of U-Net\cite{ronnebergerUNetConvolutionalNetworks2015} in the design of deep neural networks. 
Many designs of the EISR model also adopted the U-Net architecture, and the RFDN\cite{liu2020residual} is a representative example, which won the first prize in the AIM 2020 competition. 
Through the observation of the U-Net structure, we found its main structure can be transformed into mutually independent branches by deconstructing its branching structure, thereby increasing parallelism. 
Furthermore, multiple branches can be executed in parallel on the same GPU by reparameterization. 
Within the GPU's capacity, this method can reduce the number of layers that are executed serially in the network, therefore improving the inference speed.

In this paper, we proposed Parallel RFDN (PRFDN) based on the pre-trained RFDN. Our method disentangles the sequentially computed trunks in RFDN into branches and performs re-parametrization to make these branches inference in parallel on single devices. After that, we further perform pruning on the model and finetune it to achieve higher performance.

\subsection{Paper ID}
Make sure that the Paper ID from the submission system is visible in the version submitted for review (replacing the ``*****'' you see in this document).
If you are using the \LaTeX\ template, \textbf{make sure to update paper ID in the appropriate place in the tex file}.


\subsection{Methods}

We proposed Parallel RFDN (PRFDN) as is shown in figure \ref{fig:PRFDN}.
Our method consists of four stages to transform a pre-trained RFDN\cite{liu2020residual} into PRFDN.

\subsection{Branching}
To accelerate the inference, we first consider reducing the data dependency in the model to achieve higher parallelism. 
According to our observation, the main structure of U-Net can be transformed into mutually independent branches to increase parallelism. 
Our method disentangles the sequentially computed trunks of RFDN into branches. 
As is shown in figure \ref{fig:Branching}, after the branching, the major part of the model will consist of four independent branches that can calculate in parallel. 
To improve the accuracy, we also design a small SR block (SRFDB) based on the RFDB of RFDN and add them before the input of each branch. 
% This type of block 用于拟合多层RFDB的输出结果，使得输入到被展开后的RFDB block的输入与其在原始模型中接收的输入相近，从而以一个较小的代价保证模型精度
This type of block is utilized to fit the output of cascaded RFDBs, such that the input to the disentangled RFDB block closely resembles the input received in the original RFDN.
This ensures model accuracy at a reduced cost.

\subsection{Training}
Since we only change the data flow but not the structure of RFDB, the pre-trained RFDN parameters can still be loaded into the major part of our branch model (only except for those SRFDBs). 
To benefit from the pre-training, we load the pre-trained RFDN parameters into our branch model before training our branch model.
In this step, we first trained only SRFDBs for 100 epochs (on LSDIR dataset and DIV2K dataset) with freeze parameters in RFDBs, and then trained the whole model for 100 epochs.

\subsection{Re-parametrization}
Without much data dependency, branches in our model can be computed in parallel. 
However, a single GPU cannot compute two or more different models in parallel. 
To address this issue and achieve parallel computing of multiple branches on a single GPU, we should merge the four branches into a single branch. 
As is shown in figure \ref{fig:Branching}, three of these four branches have exactly the same structure with only different parameters, and the only difference between the remaining branch and the other branches is the absence of an RFDBS.
Therefore, we should 1) merge those four convolution layers at the beginning of the branches into one, 2) merge those three RFDBS into one, and 3) merge those four RFDBs into one.

In our method, we create a bigger RFDB and a bigger SRFDB with bigger convolution layers and move the weight and bias from the RFDBs and SRFDBs into them.
More specifically, we merge the convolution layers according to the following derivation.

Assuming $Y=Conv(X, K)$ represents a convolution layer,
where $K\in\mathbb{R}^{c_o\times c\times h_k\times w_k}$ is the convolution kernel; $X\in\mathbb{R}^{c\times h\times w}$ is the input of convolution layer and $Y\in\mathbb{R}^{c_o\times h'\times w'}$ is the output; 
$c$, $h$, $w$ are the channel, height, and width of the input respectively; 
$c_o$, $h'$, $w'$ are the channel, height, and width of the output respectively;
$h_k$ and $w_k$ are the height and width of the kernel respectively.
If we want to merge $n$ convolution layers $Y_i=Conv(X_i, K_i)$ into a big convolution layer $Conv_b$, this $Conv_b$ can be represented as:
\begin{equation}
  \begin{aligned}
    Y&=Conv_b([X_1,\dots,X_n], K_1,\dots,K_n)\\
    &=[Y_1,\dots,Y_n]\\
    &=\sum_{i=1}^n[\bm 0,\dots,Y_i,\dots,\bm 0]\\
    &=\sum_{i=1}^n[\bm 0,\dots,Conv(X_i, K_i),\dots,\bm 0]\\
    &=Conv([X_1,\dots,X_n], 
    \begin{bmatrix}
      K_1 & \bm 0 & \dots & \bm 0 \\
      \bm 0 & K_2 & \dots & \bm 0 \\
      \vdots & \vdots & \ddots & \vdots \\
      \bm 0 & \bm 0 & \dots & K_n \\
    \end{bmatrix})\\
    &=Conv(X_{concat}, K_{diag})
  \end{aligned}
\end{equation}

Therefore, when computing the merged convolution layer $Conv_b$, all the input $X_i, i\in[1,n]$ would be concatenated into $X_{concat}$ and all the kernel $K_i, i\in[1,n]$ would be re-parameterized into $K_{diag}$ (in first two dimensions) before performing a normal convolution operation.

Inspired by Torch-Pruning\cite{fang2023depgraph}, we implemented our re-parametrization algorithm as a convolution constructor function:
\begin{verbatim}
def merge_conv(convs, c, c_o, k, **kwargs):
  n = len(convs)
  conv = nn.Conv2d(c*n, c_o*n, k, **kwargs)
  for i, j in product(range(n), range(n)):
    if j == i:
      conv.weight[j*c_o:(j+1)*c_o,
                  i*c:(i+1)*c,
                  ...] = \
        convs[i].weight
    else:
      conv.weight[j*c_o:(j+1)*c_o,
                  i*c:(i+1)*c,
                  ...] = \
        torch.zeros(convs[i].weight.shape)
  for i in range(n):
      conv.bias[i*c_o:(i+1)*c_o] = \
        convs[i].bias
  return conv
\end{verbatim}

As is shown in figure \ref{fig:Re-parametrization}, we merge and re-parametrize the four convolution layer, three SRFDBs and four RFDBs into a bigger convolution layer, SRFDB and RFDB respectively. 
After re-parameterization, we get a bigger model that is equivalent to that of the branches of the branch model computed in parallel on a single GPU.

\subsection{Pruning}
The re-parametrized model is big. To reduce the resource consumption and further accelerate the inference, we applied selective channel-wise auto-pruning (Torch-Pruning\cite{fang2023depgraph}) on our re-parametrized model, as is shown in figure \ref{fig:Pruning}. 

Specifically, as channel-wise pruning would modify the output channel number of certain layers (especially convolution layers), channel indices of channel-wise split and concatenate operations should be adjusted accordingly.
However, determining which channels are pruned during auto-pruning is difficult to implement in code.
In our method, we propose to replace all channel-wise split and concatenate operations with equivalent 1x1 convolutions.
This allows auto-pruning to directly prune these operations and eliminates the need to adjust their channel indices separately.

After re-parametrization, we used Torch-Pruning to prune the model.
In our implementation, we prune out 90\% channels from our model in 16 steps, and between each pruning step, we finetune the model by a 10-epoch training.

\begin{figure*}
    \centering
    \begin{subfigure}[b]{0.49\linewidth}
		\centering
        \includegraphics[width=\textwidth]{../RFDN.pdf}
        \caption{RFDN}
        \label{fig:RFDN}
    \end{subfigure}
    \begin{subfigure}[b]{0.49\linewidth}
		\centering
        \includegraphics[width=\textwidth]{../Branching.pdf}
        \caption{Branching}
        \label{fig:Branching}
    \end{subfigure}
    \begin{subfigure}[b]{0.49\linewidth}
		\centering
        \includegraphics[width=\textwidth]{../Re-parametrization.pdf}
        \caption{Re-parametrization}
        \label{fig:Re-parametrization}
    \end{subfigure}
    \begin{subfigure}[b]{0.49\linewidth}
		\centering
        \includegraphics[width=\textwidth]{../Pruning.pdf}
        \caption{Pruning}
        \label{fig:Pruning}
    \end{subfigure}
    \caption{Transform a pre-trained RFDN into PRFDN}
    \label{fig:PRFDN}
\end{figure*}

\noindent
FAQ\medskip\\
{\bf Q:} Are acknowledgements OK?\\
{\bf A:} No.  Leave them for the final copy.\medskip\\
{\bf Q:} How do I cite my results reported in open challenges?
{\bf A:} To conform with the double-blind review policy, you can report results of other challenge participants together with your results in your paper.
For your results, however, you should not identify yourself and should not mention your participation in the challenge.
Instead present your results referring to the method proposed in your paper and draw conclusions based on the experimental comparison to other results.\medskip\\

%-------------------------------------------------------------------------
\subsection{References}

List and number all bibliographical references in 9-point Times, single-spaced, at the end of your paper.
When referenced in the text, enclose the citation number in square brackets, for
example~\cite{Authors14}.
Where appropriate, include page numbers and the name(s) of editors of referenced books.
When you cite multiple papers at once, please make sure that you cite them in numerical order like this \cite{Alpher02,Alpher03,Alpher05,Authors14b,Authors14}.
If you use the template as advised, this will be taken care of automatically.

\begin{table}
  \centering
  \begin{tabular}{@{}lc@{}}
    \toprule
    Method & Frobnability \\
    \midrule
    Theirs & Frumpy \\
    Yours & Frobbly \\
    Ours & Makes one's heart Frob\\
    \bottomrule
  \end{tabular}
  \caption{Results.   Ours is better.}
  \label{tab:example}
\end{table}

%------------------------------------------------------------------------
\section{Final copy}

You must include your signed IEEE copyright release form when you submit your finished paper.
We MUST have this form before your paper can be published in the proceedings.

Please direct any questions to the production editor in charge of these proceedings at the IEEE Computer Society Press:
\url{https://www.computer.org/about/contact}.


%%%%%%%%% REFERENCES
{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}

\end{document}
