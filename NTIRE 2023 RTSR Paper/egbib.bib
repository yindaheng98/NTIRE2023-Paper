@inproceedings{liu2020residual,
title={Residual feature distillation network for lightweight image super-resolution},
author={Liu, Jie and Tang, Jie and Wu, Gangshan},
booktitle={Computer Vision--ECCV 2020 Workshops: Glasgow, UK, August 23--28, 2020, Proceedings, Part III 16},
pages={41--55},
year={2020},
organization={Springer}
}
@article{fang2023depgraph,
  title={DepGraph: Towards Any Structural Pruning},
  author={Fang, Gongfan and Ma, Xinyin and Song, Mingli and Mi, Michael Bi and Wang, Xinchao},
  journal={The Thirty-Fourth IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  year={2023}
}
@inproceedings{10.1145/2751496.2751502,
author = {Jin, Tianxing and He, Songtao and Liu, Yunxin},
title = {Towards Accurate GPU Power Modeling for Smartphones},
year = {2015},
isbn = {9781450334990},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2751496.2751502},
doi = {10.1145/2751496.2751502},
abstract = {With the increasingly high power consumption of smartphone GPUs, accurate GPU power modeling is desirable for mobile game developers to optimize the power performance of their game code. However, existing GPU power models for smartphones simply use only GPU utilization to estimate GPU power consumption. In this paper, we observe that GPU utilization fails to capture real usage of modern mobile GPU hardware and thus has a high estimation error on modern smartphones. We discover that the root cause is that different types of GPU operations may consume very different amount of power even they have the same GPU utilization. To improve the accuracy of GPU power modeling, we propose to consider more fine-grained predicators, including vertex-processing load and pixel-processing load, in modeling GPU power consumption. We report how to build such a new GPU model for commercial smartphones and evaluate it using various benchmarks and mobile games. Experimental results show that compared to existing utilization-based model, our new model is able to significantly reduce the maximum modeling error from 14.8% to 6.5%.},
booktitle = {Proceedings of the 2nd Workshop on Mobile Gaming},
pages = {7–11},
numpages = {5},
keywords = {power modeling, mobile gpus, performance counters},
location = {Florence, Italy},
series = {MobiGames '15}
}

@inproceedings{kimNeuralEnhancedLiveStreaming2020,
  title = {Neural-{{Enhanced Live Streaming}}: {{Improving Live Video Ingest}} via {{Online Learning}}},
  shorttitle = {Neural-{{Enhanced Live Streaming}}},
  booktitle = {Proceedings of the {{Annual}} Conference of the {{ACM Special Interest Group}} on {{Data Communication}} on the Applications, Technologies, Architectures, and Protocols for Computer Communication},
  author = {Kim, Jaehong and Jung, Youngmok and Yeo, Hyunho and Ye, Juncheol and Han, Dongsu},
  year = {2020},
  month = jul,
  pages = {107--125},
  publisher = {{ACM}},
  address = {{Virtual Event USA}},
  doi = {10.1145/3387514.3405856},
  urldate = {2021-01-24},
  isbn = {978-1-4503-7955-7},
  langid = {english},
  keywords = {已读},
  file = {C\:\\Users\\yinda\\Zotero\\storage\\YYAPP8AX\\Kim 等。 - 2020 - Neural-Enhanced Live Streaming Improving Live Vid.pdf}
}

@inproceedings{yeoNEMOEnablingNeuralenhanced2020,
  ids = {DBLP:conf/mobicom/YeoCJYH20,NEMOEnablingNeuralenhancedVideo2020a},
  title = {{{NEMO}}: Enabling Neural-Enhanced Video Streaming on Commodity Mobile Devices},
  shorttitle = {{{NEMO}}},
  booktitle = {Proceedings of the 26th {{Annual International Conference}} on {{Mobile Computing}} and {{Networking}}},
  author = {Yeo, Hyunho and Chong, Chan Ju and Jung, Youngmok and Ye, Juncheol and Han, Dongsu},
  year = {2020},
  month = sep,
  pages = {1--14},
  publisher = {{ACM}},
  address = {{London United Kingdom}},
  doi = {10.1145/3372224.3419185},
  urldate = {2021-01-24},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl = {https://dblp.org/rec/conf/mobicom/YeoCJYH20.bib},
  isbn = {978-1-4503-7085-1},
  langid = {english},
  keywords = {已读},
  timestamp = {Mon, 30 Nov 2020 00:00:00 +0100},
  file = {C\:\\Users\\yinda\\Zotero\\storage\\64XKHCIV\\Yeo 等。 - 2020 - NEMO enabling neural-enhanced video streaming on .pdf}
}

@inproceedings{yeoNeuralAdaptiveContentaware2018,
  title = {Neural {{Adaptive Content-aware Internet Video Delivery}}},
  booktitle = {13th \{\vphantom\}{{USENIX}}\vphantom\{\} {{Symposium}} on {{Operating Systems Design}} and {{Implementation}} (\{\vphantom\}{{OSDI}}\vphantom\{\} 18)},
  author = {Yeo, Hyunho and Jung, Youngmok and Kim, Jaehong and Shin, Jinwoo and Han, Dongsu},
  year = {2018},
  pages = {645--661},
  urldate = {2021-01-26},
  isbn = {978-1-939133-08-3},
  langid = {english},
  keywords = {已读},
  file = {C\:\\Users\\yinda\\Zotero\\storage\\MGF96PJK\\Yeo 等。 - 2018 - Neural Adaptive Content-aware Internet Video Deliv.pdf;C\:\\Users\\yinda\\Zotero\\storage\\ADRX5YBS\\yeo.html}
}

@inproceedings{maoNeuralAdaptiveVideo2017,
  title = {Neural {{Adaptive Video Streaming}} with {{Pensieve}}},
  booktitle = {Proceedings of the {{Conference}} of the {{ACM Special Interest Group}} on {{Data Communication}}},
  author = {Mao, Hongzi and Netravali, Ravi and Alizadeh, Mohammad},
  year = {2017},
  month = aug,
  series = {{{SIGCOMM}} '17},
  pages = {197--210},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/3098822.3098843},
  urldate = {2022-06-18},
  abstract = {Client-side video players employ adaptive bitrate (ABR) algorithms to optimize user quality of experience (QoE). Despite the abundance of recently proposed schemes, state-of-the-art ABR algorithms suffer from a key limitation: they use fixed control rules based on simplified or inaccurate models of the deployment environment. As a result, existing schemes inevitably fail to achieve optimal performance across a broad set of network conditions and QoE objectives. We propose Pensieve, a system that generates ABR algorithms using reinforcement learning (RL). Pensieve trains a neural network model that selects bitrates for future video chunks based on observations collected by client video players. Pensieve does not rely on pre-programmed models or assumptions about the environment. Instead, it learns to make ABR decisions solely through observations of the resulting performance of past decisions. As a result, Pensieve automatically learns ABR algorithms that adapt to a wide range of environments and QoE metrics. We compare Pensieve to state-of-the-art ABR algorithms using trace-driven and real world experiments spanning a wide variety of network conditions, QoE metrics, and video properties. In all considered scenarios, Pensieve outperforms the best state-of-the-art scheme, with improvements in average QoE of 12\%--25\%. Pensieve also generalizes well, outperforming existing schemes even on networks for which it was not explicitly trained.},
  isbn = {978-1-4503-4653-5},
  keywords = {bitrate adaptation,reinforcement learning,video streaming},
  file = {C\:\\Users\\yinda\\Zotero\\storage\\P99RA9RI\\Mao 等。 - 2017 - Neural Adaptive Video Streaming with Pensieve.pdf}
}

@inproceedings{mehtaEVRNetEfficientVideo2021,
  title = {{{EVRNet}}: {{Efficient Video Restoration}} on {{Edge Devices}}},
  shorttitle = {{{EVRNet}}},
  booktitle = {Proceedings of the 29th {{ACM International Conference}} on {{Multimedia}}},
  author = {Mehta, Sachin and Kumar, Amit and Reda, Fitsum and Nasery, Varun and Mulukutla, Vikram and Ranjan, Rakesh and Chandra, Vikas},
  year = {2021},
  month = oct,
  series = {{{MM}} '21},
  pages = {983--992},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/3474085.3475477},
  urldate = {2022-09-12},
  abstract = {In video transmission applications, video signals are transmitted over lossy channels, resulting in low-quality received signals. To re- store videos on recipient edge devices in real-time, we introduce an efficient video restoration network, EVRNet. EVRNet efficiently allocates parameters inside the network using alignment, differential, and fusion modules. With extensive experiments on different video restoration tasks (deblocking, denoising, and super-resolution), we demonstrate that EVRNet delivers competitive performance to existing methods with significantly fewer parameters and MACs. For example, EVRNet has 260\texttimes{} fewer parameters and 958\texttimes{} fewer MACs than enhanced deformable convolution-based video restoration net- work (EDVR) for 4\texttimes{} video super-resolution while its SSIM score is 0.018 less than EDVR. We also evaluated the performance of EVR-Net under multiple distortions on unseen dataset to demonstrate its ability in modeling variable-length sequences under both camera and object motion.},
  isbn = {978-1-4503-8651-7},
  keywords = {convolutional neural network,edge devices,on-device,super-resolution,video decompression,video denoising,video restoration,待阅},
  file = {C\:\\Users\\yinda\\Zotero\\storage\\LH9GIMU7\\Mehta 等。 - 2021 - EVRNet Efficient Video Restoration on Edge Device.pdf}
}

@inproceedings{duFastMemoryEfficientNetwork2022,
  title = {Fast and {{Memory-Efficient Network Towards Efficient Image Super-Resolution}}},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Du, Zongcai and Liu, Ding and Liu, Jie and Tang, Jie and Wu, Gangshan and Fu, Lean},
  year = {2022},
  pages = {853--862},
  urldate = {2023-03-06},
  langid = {english},
  file = {C\:\\Users\\yinda\\Zotero\\storage\\6HNPSE7Y\\Du 等 - 2022 - Fast and Memory-Efficient Network Towards Efficien.pdf}
}

@article{howardMobileNetsEfficientConvolutional2017,
  title = {{{MobileNets}}: {{Efficient Convolutional Neural Networks}} for {{Mobile Vision Applications}}},
  shorttitle = {{{MobileNets}}},
  author = {Howard, Andrew G. and Zhu, Menglong and Chen, Bo and Kalenichenko, Dmitry and Wang, Weijun and Weyand, Tobias and Andreetto, Marco and Adam, Hartwig},
  year = {2017},
  month = apr,
  journal = {arXiv:1704.04861 [cs]},
  eprint = {1704.04861},
  primaryclass = {cs},
  urldate = {2022-02-17},
  abstract = {We present a class of efficient models called MobileNets for mobile and embedded vision applications. MobileNets are based on a streamlined architecture that uses depth-wise separable convolutions to build light weight deep neural networks. We introduce two simple global hyper-parameters that efficiently trade off between latency and accuracy. These hyper-parameters allow the model builder to choose the right sized model for their application based on the constraints of the problem. We present extensive experiments on resource and accuracy tradeoffs and show strong performance compared to other popular models on ImageNet classification. We then demonstrate the effectiveness of MobileNets across a wide range of applications and use cases including object detection, finegrain classification, face attributes and large scale geo-localization.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\yinda\\Zotero\\storage\\ZT2R8W4K\\Howard 等。 - 2017 - MobileNets Efficient Convolutional Neural Network.pdf;C\:\\Users\\yinda\\Zotero\\storage\\YJALUU59\\1704.html}
}

@article{kongClassSRGeneralFramework2021,
  ids = {ClassSRGeneralFrameworkAccelerate2021a},
  title = {{{ClassSR}}: {{A General Framework}} to {{Accelerate Super-Resolution Networks}} by {{Data Characteristic}}},
  shorttitle = {{{ClassSR}}},
  author = {Kong, Xiangtao and Zhao, Hengyuan and Qiao, Yu and Dong, Chao},
  year = {2021},
  month = mar,
  journal = {arXiv:2103.04039 [cs]},
  eprint = {2103.04039},
  primaryclass = {cs},
  urldate = {2021-05-07},
  abstract = {We aim at accelerating super-resolution (SR) networks on large images (2K-8K). The large images are usually decomposed into small sub-images in practical usages. Based on this processing, we found that different image regions have different restoration difficulties and can be processed by networks with different capacities. Intuitively, smooth areas are easier to super-solve than complex textures. To utilize this property, we can adopt appropriate SR networks to process different sub-images after the decomposition. On this basis, we propose a new solution pipeline -- ClassSR that combines classification and SR in a unified framework. In particular, it first uses a Class-Module to classify the sub-images into different classes according to restoration difficulties, then applies an SR-Module to perform SR for different classes. The Class-Module is a conventional classification network, while the SR-Module is a network container that consists of the to-be-accelerated SR network and its simplified versions. We further introduce a new classification method with two losses -- Class-Loss and Average-Loss to produce the classification results. After joint training, a majority of sub-images will pass through smaller networks, thus the computational cost can be significantly reduced. Experiments show that our ClassSR can help most existing methods (e.g., FSRCNN, CARN, SRResNet, RCAN) save up to 50\% FLOPs on DIV8K datasets. This general framework can also be applied in other low-level vision tasks.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,已读},
  file = {C\:\\Users\\yinda\\Zotero\\storage\\92P9N6TI\\Kong 等。 - 2021 - ClassSR A General Framework to Accelerate Super-R.pdf;C\:\\Users\\yinda\\Zotero\\storage\\W82BCP3J\\2103.html}
}

@inproceedings{leeMobiSREfficientOnDevice2019,
  title = {{{MobiSR}}: {{Efficient On-Device Super-Resolution}} through {{Heterogeneous Mobile Processors}}},
  shorttitle = {{{MobiSR}}},
  booktitle = {The 25th {{Annual International Conference}} on {{Mobile Computing}} and {{Networking}}},
  author = {Lee, Royson and Venieris, Stylianos I. and Dudziak, Lukasz and Bhattacharya, Sourav and Lane, Nicholas D.},
  year = {2019},
  month = oct,
  series = {{{MobiCom}} '19},
  pages = {1--16},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/3300061.3345455},
  urldate = {2021-11-29},
  abstract = {In recent years, convolutional networks have demonstrated unprecedented performance in the image restoration task of super-resolution (SR). SR entails the upscaling of a single low-resolution image in order to meet application-specific image quality demands and plays a key role in mobile devices. To comply with privacy regulations and reduce the overhead of cloud computing, executing SR models locally on-device constitutes a key alternative approach. Nevertheless, the excessive compute and memory requirements of SR workloads pose a challenge in mapping SR networks on resource-constrained mobile platforms. This work presents MobiSR, a novel framework for performing efficient super-resolution on-device. Given a target mobile platform, the proposed framework considers popular model compression techniques and traverses the design space to reach the highest performing trade-off between image quality and processing speed. At run time, a novel scheduler dispatches incoming image patches to the appropriate model-engine pair based on the patch's estimated upscaling difficulty in order to meet the required image quality with minimum processing latency. Quantitative evaluation shows that the proposed framework yields on-device SR designs that achieve an average speedup of 2.13x over highly-optimized parallel difficulty-unaware mappings and 4.79x over highly-optimized single compute engine implementations.},
  isbn = {978-1-4503-6169-9},
  keywords = {deep neural networks,heterogeneous computing,mobile computing,scheduling,super-resolution},
  file = {C\:\\Users\\yinda\\Zotero\\storage\\WIJDNLDK\\Lee 等。 - 2019 - MobiSR Efficient On-Device Super-Resolution throu.pdf}
}

@inproceedings{liNTIRE2022Challenge2022,
  title = {{{NTIRE}} 2022 {{Challenge}} on {{Efficient Super-Resolution}}: {{Methods}} and {{Results}}},
  shorttitle = {{{NTIRE}} 2022 {{Challenge}} on {{Efficient Super-Resolution}}},
  booktitle = {2022 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition Workshops}} ({{CVPRW}})},
  author = {Li, Yawei and Zhang, Kai and Timofte, Radu and Van Gool, Luc and Kong, Fangyuan and Li, Mingxi and Liu, Songwei and Du, Zongcai and Liu, Ding and Zhou, Chenhui and Chen, Jingyi and Han, Qingrui and Li, Zheyuan and Liu, Yingqi and Chen, Xiangyu and Cai, Haoming and Qiao, Yu and Dong, Chao and Sun, Long and Pan, Jinshan and Zhu, Yi and Zong, Zhikai and Liu, Xiaoxiao and Hui, Zheng and Yang, Tao and Ren, Peiran and Xie, Xuansong and Hua, Xian-Sheng and Wang, Yanbo and Ji, Xiaozhong and Lin, Chuming and Luo, Donghao and Tai, Ying and Wang, Chengjie and Zhang, Zhizhong and Xie, Yuan and Cheng, Shen and Luo, Ziwei and Yu, Lei and Wen, Zhihong and Wul, Qi and Li, Youwei and Fan, Haoqiang and Sun, Jian and Liu, Shuaicheng and Huang, Yuanfei and Jin, Meiguang and Huang, Hua and Liu, Jing and Zhang, Xinjian and Wang, Yan and Long, Lingshun and Li, Gen and Zhang, Yuanfan and Cao, Zuowei and Sun, Lei and Alexander, Panaetov and Wang, Yucong and Cai, Minjie and Wang, Li and Tian, Lu and Wang, Zheyuan and Ma, Hongbing and Liu, Jie and Chen, Chao and Cai, Yidong and Tang, Jie and Wu, Gangshan and Wang, Weiran and Huang, Shirui and Lu, Honglei and Liu, Huan and Wang, Keyan and Chen, Jun and Chen, Shi and Miao, Yuchun and Huang, Zimo and Zhang, Lefei and Ayazo{\u g}lu, Mustafa and Xiong, Wei and Xiong, Chengyi and Wang, Fei and Li, Hao and Wen, Ruimian and Yang, Zhijing and Zou, Wenbin and Zheng, Weixin and Ye, Tian and Zhang, Yuncheng and Kong, Xiangzhen and Arora, Aditya and Zamir, Syed Waqas and Khan, Salman and Hayat, Munawar and Khan, Fahad Shahbaz and Gao, Dandan and Zhou, Dengwen and Ning, Qian and Tang, Jingzhu and Huang, Han and Wang, Yufei and Peng, Zhangheng and Li, Haobo and Guan, Wenxue and Gong, Shenghua and Li, Xin and Liu, Jun and Wang, Wanjun and Zhou, Dengwen and Zeng, Kun and Lin, Hanjiang and Chen, Xinyu and Fang, Jinsheng},
  year = {2022},
  month = jun,
  pages = {1061--1101},
  issn = {2160-7516},
  doi = {10.1109/CVPRW56347.2022.00118},
  abstract = {This paper reviews the NTIRE 2022 challenge on efficient single image super-resolution with focus on the proposed solutions and results. The task of the challenge was to super-resolve an input image with a magnification factor of \texttimes 4 based on pairs of low and corresponding high resolution images. The aim was to design a network for single image super-resolution that achieved improvement of efficiency measured according to several metrics including runtime, parameters, FLOPs, activations, and memory consumption while at least maintaining the PSNR of 29.00dB on DIV2K validation set. IMDN is set as the baseline for efficiency measurement. The challenge had 3 tracks including the main track (runtime), sub-track one (model complexity), and sub-track two (overall performance). In the main track, the practical runtime performance of the submissions was evaluated. The rank of the teams were determined directly by the absolute value of the average runtime on the validation set and test set. In sub-track one, the number of parameters and FLOPs were considered. And the individual rankings of the two metrics were summed up to determine a final ranking in this track. In sub-track two, all of the five metrics mentioned in the description of the challenge including runtime, parameter count, FLOPs, activations, and memory consumption were considered. Similar to sub-track one, the rankings of five metrics were summed up to determine a final ranking. The challenge had 303 registered participants, and 43 teams made valid submissions. They gauge the state-of-the-art in efficient single image super-resolution.},
  keywords = {Conferences,Convolution,Graphics processing units,Measurement,Memory management,Runtime,Superresolution},
  file = {C\:\\Users\\yinda\\Zotero\\storage\\IQFU5G69\\Li 等 - 2022 - NTIRE 2022 Challenge on Efficient Super-Resolution.pdf;C\:\\Users\\yinda\\Zotero\\storage\\FXD8AT2V\\9857089.html}
}

@article{liuSplitSREndtoEndApproach2021,
  title = {{{SplitSR}}: {{An End-to-End Approach}} to {{Super-Resolution}} on {{Mobile Devices}}},
  shorttitle = {{{SplitSR}}},
  author = {Liu, Xin and Li, Yuang and Fromm, Josh and Wang, Yuntao and Jiang, Ziheng and Mariakakis, Alex and Patel, Shwetak},
  year = {2021},
  month = jan,
  journal = {arXiv:2101.07996 [cs]},
  eprint = {2101.07996},
  primaryclass = {cs},
  urldate = {2021-11-23},
  abstract = {Super-resolution (SR) is a coveted image processing technique for mobile apps ranging from the basic camera apps to mobile health. Existing SR algorithms rely on deep learning models with significant memory requirements, so they have yet to be deployed on mobile devices and instead operate in the cloud to achieve feasible inference time. This shortcoming prevents existing SR methods from being used in applications that require near real-time latency. In this work, we demonstrate state-of-the-art latency and accuracy for on-device super-resolution using a novel hybrid architecture called SplitSR and a novel lightweight residual block called SplitSRBlock. The SplitSRBlock supports channel-splitting, allowing the residual blocks to retain spatial information while reducing the computation in the channel dimension. SplitSR has a hybrid design consisting of standard convolutional blocks and lightweight residual blocks, allowing people to tune SplitSR for their computational budget. We evaluate our system on a low-end ARM CPU, demonstrating both higher accuracy and up to 5 times faster inference than previous approaches. We then deploy our model onto a smartphone in an app called ZoomSR to demonstrate the first-ever instance of on-device, deep learning-based SR. We conducted a user study with 15 participants to have them assess the perceived quality of images that were post-processed by SplitSR. Relative to bilinear interpolation -- the existing standard for on-device SR -- participants showed a statistically significant preference when looking at both images (Z=-9.270, p{$<$}0.01) and text (Z=-6.486, p{$<$}0.01).},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Human-Computer Interaction,已读},
  file = {C\:\\Users\\yinda\\Zotero\\storage\\Z7QBL5QV\\Liu 等。 - 2021 - SplitSR An End-to-End Approach to Super-Resolutio.pdf;C\:\\Users\\yinda\\Zotero\\storage\\KQSIEET2\\2101.html}
}

@article{sandlerMobileNetV2InvertedResiduals2019,
  title = {{{MobileNetV2}}: {{Inverted Residuals}} and {{Linear Bottlenecks}}},
  shorttitle = {{{MobileNetV2}}},
  author = {Sandler, Mark and Howard, Andrew and Zhu, Menglong and Zhmoginov, Andrey and Chen, Liang-Chieh},
  year = {2019},
  month = mar,
  journal = {arXiv:1801.04381 [cs]},
  eprint = {1801.04381},
  primaryclass = {cs},
  urldate = {2022-02-17},
  abstract = {In this paper we describe a new mobile architecture, MobileNetV2, that improves the state of the art performance of mobile models on multiple tasks and benchmarks as well as across a spectrum of different model sizes. We also describe efficient ways of applying these mobile models to object detection in a novel framework we call SSDLite. Additionally, we demonstrate how to build mobile semantic segmentation models through a reduced form of DeepLabv3 which we call Mobile DeepLabv3. The MobileNetV2 architecture is based on an inverted residual structure where the input and output of the residual block are thin bottleneck layers opposite to traditional residual models which use expanded representations in the input an MobileNetV2 uses lightweight depthwise convolutions to filter features in the intermediate expansion layer. Additionally, we find that it is important to remove non-linearities in the narrow layers in order to maintain representational power. We demonstrate that this improves performance and provide an intuition that led to this design. Finally, our approach allows decoupling of the input/output domains from the expressiveness of the transformation, which provides a convenient framework for further analysis. We measure our performance on Imagenet classification, COCO object detection, VOC image segmentation. We evaluate the trade-offs between accuracy, and number of operations measured by multiply-adds (MAdd), as well as the number of parameters},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\yinda\\Zotero\\storage\\UEG39H2I\\Sandler 等。 - 2019 - MobileNetV2 Inverted Residuals and Linear Bottlen.pdf;C\:\\Users\\yinda\\Zotero\\storage\\S8IXV36G\\1801.html}
}

@inproceedings{ronnebergerUNetConvolutionalNetworks2015,
  title = {U-{{Net}}: {{Convolutional Networks}} for {{Biomedical Image Segmentation}}},
  shorttitle = {U-{{Net}}},
  booktitle = {Medical {{Image Computing}} and {{Computer-Assisted Intervention}} \textendash{} {{MICCAI}} 2015},
  author = {Ronneberger, Olaf and Fischer, Philipp and Brox, Thomas},
  editor = {Navab, Nassir and Hornegger, Joachim and Wells, William M. and Frangi, Alejandro F.},
  year = {2015},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {234--241},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-319-24574-4_28},
  abstract = {There is large consent that successful training of deep networks requires many thousand annotated training samples. In this paper, we present a network and training strategy that relies on the strong use of data augmentation to use the available annotated samples more efficiently. The architecture consists of a contracting path to capture context and a symmetric expanding path that enables precise localization. We show that such a network can be trained end-to-end from very few images and outperforms the prior best method (a sliding-window convolutional network) on the ISBI challenge for segmentation of neuronal structures in electron microscopic stacks. Using the same network trained on transmitted light microscopy images (phase contrast and DIC) we won the ISBI cell tracking challenge 2015 in these categories by a large margin. Moreover, the network is fast. Segmentation of a 512x512 image takes less than a second on a recent GPU. The full implementation (based on Caffe) and the trained networks are available at http://lmb.informatik.uni-freiburg.de/people/ronneber/u-net.},
  isbn = {978-3-319-24574-4},
  langid = {english},
  keywords = {Convolutional Layer,Data Augmentation,Deep Network,Ground Truth Segmentation,Training Image},
  file = {C\:\\Users\\yinda\\Zotero\\storage\\BJTX9FCK\\Ronneberger 等。 - 2015 - U-Net Convolutional Networks for Biomedical Image.pdf}
}

@inproceedings{khaniRealTimeVideoInference2021,
  title = {Real-{{Time Video Inference}} on {{Edge Devices}} via {{Adaptive Model Streaming}}},
  booktitle = {2021 {{IEEE}}/{{CVF International Conference}} on {{Computer Vision}} ({{ICCV}})},
  author = {Khani, Mehrdad and Hamadanian, Pouya and {Nasr-Esfahany}, Arash and Alizadeh, Mohammad},
  year = {2021},
  pages = {4552--4562},
  issn = {2380-7504},
  doi = {10.1109/ICCV48922.2021.00453},
  abstract = {Real-time video inference on edge devices like mobile phones and drones is challenging due to the high computation cost of Deep Neural Networks. We present Adaptive Model Streaming (AMS), a new approach to improving the performance of efficient lightweight models for video inference on edge devices. AMS uses a remote server to continually train and adapt a small model running on the edge device, boosting its performance on the live video using online knowledge distillation from a large, state-of-the-art model. We discuss the challenges of over-the-network model adaptation for video inference and present several techniques to reduce communication the cost of this approach: avoiding excessive overfitting, updating a small fraction of important model parameters, and adaptive sampling of training frames at edge devices. On the task of video semantic segmentation, our experimental results show 0.4\textendash 17.8 percent mean Intersection-over-Union improvement compared to a pretrained model across several video datasets. Our prototype can perform video segmentation at 30 frames-per-second with 40 milliseconds camera-to-label latency on a Samsung Galaxy S10+ mobile phone, using less than 300 Kbps uplink and downlink bandwidth on the device.},
  keywords = {Adaptation models,Bandwidth,Computational modeling,Efficient training and inference methods,grouping and shape,Motion and tracking,Performance evaluation,Segmentation,Semantics,Streaming media,Training,Vision applications and systems},
  file = {C\:\\Users\\yinda\\Zotero\\storage\\9AKZLXLU\\Khani 等。 - 2021 - Real-Time Video Inference on Edge Devices via Adap.pdf}
}

@inproceedings{taoCompressionGenerativePretrained2022,
  title = {Compression of {{Generative Pre-trained Language Models}} via {{Quantization}}},
  booktitle = {Proceedings of the 60th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  author = {Tao, Chaofan and Hou, Lu and Zhang, Wei and Shang, Lifeng and Jiang, Xin and Liu, Qun and Luo, Ping and Wong, Ngai},
  year = {2022},
  pages = {4821--4836},
  publisher = {{Association for Computational Linguistics}},
  address = {{Dublin, Ireland}},
  doi = {10.18653/v1/2022.acl-long.331},
  urldate = {2022-10-15},
  abstract = {The increasing size of generative Pre-trained Language Models (PLMs) have greatly increased the demand for model compression. Despite various methods to compress BERT or its variants, there are few attempts to compress generative PLMs, and the underlying difficulty remains unclear. In this paper, we compress generative PLMs by quantization. We find that previous quantization methods fail on generative tasks due to the homogeneous word embeddings caused by reduced capacity and the varied distribution of weights. Correspondingly, we propose a token-level contrastive distillation to learn distinguishable word embeddings, and a module-wise dynamic scaling to make quantizers adaptive to different modules. Empirical results on various tasks show that our proposed method outperforms the state-of-the-art compression methods on generative PLMs by a clear margin. With comparable performance with the full-precision models, we achieve 14.4x and 13.4x compression rate on GPT-2 and BART, respectively.},
  keywords = {待阅},
  file = {C\:\\Users\\yinda\\Zotero\\storage\\Q3MFENMY\\Tao 等。 - 2022 - Compression of Generative Pre-trained Language Mod.pdf}
}

@inproceedings{zhangDataFreeKnowledgeDistillation2021,
  title = {Data-{{Free Knowledge Distillation}} for {{Image Super-Resolution}}},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Zhang, Yiman and Chen, Hanting and Chen, Xinghao and Deng, Yiping and Xu, Chunjing and Wang, Yunhe},
  year = {2021},
  pages = {7852--7861},
  urldate = {2021-06-26},
  langid = {english},
  file = {C\:\\Users\\yinda\\Zotero\\storage\\C7GLCHKI\\Zhang 等。 - 2021 - Data-Free Knowledge Distillation for Image Super-R.pdf;C\:\\Users\\yinda\\Zotero\\storage\\TIPMRVQ9\\Zhang_Data-Free_Knowledge_Distillation_for_Image_Super-Resolution_CVPR_2021_paper.html}
}
